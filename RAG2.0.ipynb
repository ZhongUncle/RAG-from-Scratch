{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2db84a35-8914-429e-ac16-6557ec9a03c2",
   "metadata": {},
   "source": [
    "åœ¨ä¸Šä¸€ç¯‡ä¸­ï¼Œæˆ‘ä»¬æ‰‹åŠ¨å®Œæˆäº† RAG çš„å·¥ä½œæµç¨‹ï¼Œä½†æ˜¯å®é™…ä½¿ç”¨å½“ä¸­ï¼Œæˆ‘ä»¬éœ€è¦å®ƒè‡ªåŠ¨è¿›è¡Œã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†ä¿®æ”¹ä»£ç ï¼Œè®©è¿™ä¸ªå·¥ä½œæµç¨‹è‡ªåŠ¨åŒ–è¿è¡Œã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5138808a-8c22-4cdc-84be-6beff5abb65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import gc\n",
    "import ast\n",
    "import time\n",
    "import base64\n",
    "import ollama\n",
    "import numpy as np\n",
    "from io import BytesIO\n",
    "from pdf2image import convert_from_path\n",
    "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed\n",
    "\n",
    "\n",
    "# ====================== Model Configuration ======================\n",
    "OCR_MODEL = \"glm-ocr\"\n",
    "OCR_DPI = 100\n",
    "OCR_PROMPT = \"Text recognition. Accurately extract all text, LaTeX formulas, and symbols from the image. Output only the original content.\"\n",
    "OLLAMA_HOST = \"http://localhost:11434\"\n",
    "\n",
    "LLM_MODEL = \"gpt-oss:20b\"\n",
    "\n",
    "EMBEDDING_MODEL = \"embeddinggemma\"\n",
    "\n",
    "MAX_WORKERS = 16  # The Threads of Embedding. Suit your CPU. Higher is better.\n",
    "\n",
    "# ====================== Image to Base64 ======================\n",
    "def image_to_base64(image):\n",
    "    buffered = BytesIO()\n",
    "    image.save(buffered, format=\"PNG\", quality=80, optimize=True)\n",
    "    return base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "# ====================== OCR single page ======================\n",
    "def ocr_single_page(page_img):\n",
    "    img_base64 = image_to_base64(page_img)\n",
    "    response = ollama.chat(\n",
    "        model=OCR_MODEL,\n",
    "        messages=[{'role': 'user', 'content': OCR_PROMPT, 'images': [img_base64]}],\n",
    "    )\n",
    "    return response[\"message\"][\"content\"].strip()\n",
    "\n",
    "def ocr_single_pdf(pdf_path):\n",
    "    full_text = \"\"\n",
    "    try:\n",
    "        # Get PDF file name\n",
    "        pdf_file_name = os.path.basename(pdf_path)\n",
    "        \n",
    "        print(f\"\\nğŸš€ GLM-OCR processing: {pdf_file_name}\")\n",
    "        \n",
    "        # Convert PDF to images\n",
    "        pages = convert_from_path(\n",
    "            pdf_path, dpi=OCR_DPI, thread_count=1, use_pdftocairo=True, grayscale=True\n",
    "        )\n",
    "        total_pages = len(pages)\n",
    "    \n",
    "        # Iterate through each page to process OCR\n",
    "        for idx, page_img in enumerate(pages):\n",
    "            page_text = \"\"\n",
    "            # Retry mechanism (max 2 attempts)\n",
    "            for retry in range(2):\n",
    "                try:\n",
    "                    # OCR \n",
    "                    page_text = ocr_single_page(page_img)\n",
    "                    break  # Exit retry if successful\n",
    "                except TimeoutError:\n",
    "                    print(f\"   âš ï¸ Page {idx+1} OCR timed out, retrying {retry+1}...\")\n",
    "                    time.sleep(2)\n",
    "                except Exception as e:\n",
    "                    print(f\"   âš ï¸ Page {idx+1} OCR failed: {e}\")\n",
    "                    time.sleep(2)\n",
    "            \n",
    "            # Fill placeholder text when OCR fails\n",
    "            if not page_text:\n",
    "                page_text = f\"[Recognition failed for Page {idx+1}]\"\n",
    "            \n",
    "            # Concatenate text\n",
    "            full_text += page_text + \"\\n\\n\"\n",
    "            \n",
    "    \n",
    "        # Clean up redundant whitespace characters\n",
    "        full_text = re.sub(r'\\s+', ' ', full_text).strip()\n",
    "        print(f\"âœ… OCR completed! Text length: {len(full_text)} characters\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ OCR failed: {str(e)}\")\n",
    "        full_text = \"\"\n",
    "\n",
    "    return full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5eee84f5-f838-42b9-9566-32bee5e45edd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./pdfs/Ubuntuå¦‚ä½•ç›´æ¥æ‰‹åŠ¨æ§åˆ¶é£æ‰‡é€Ÿåº¦.pdf',\n",
       " './pdfs/Guide of developing Tang Nano FPGA on Mac.pdf']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pdf_path = \"./pdfs/Guide of developing Tang Nano FPGA on Mac.pdf\"\n",
    "# ocr_single_pdf(pdf_path)\n",
    "\n",
    "def list_all_files_basic(path):\n",
    "    extensions=['.pdf']\n",
    "    # æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"é”™è¯¯ï¼šè·¯å¾„ '{path}' ä¸å­˜åœ¨\")\n",
    "        return []\n",
    "    \n",
    "    extensions_lower = [ext.lower() for ext in extensions]\n",
    "    file_list = []\n",
    "    \n",
    "    # éå†è·¯å¾„ä¸‹çš„æ‰€æœ‰æ–‡ä»¶\n",
    "    for item in os.listdir(path):\n",
    "        item_full_path = os.path.join(path, item)\n",
    "        if os.path.isfile(item_full_path):\n",
    "            # è·å–æ–‡ä»¶åç¼€å¹¶è½¬ä¸ºå°å†™\n",
    "            file_ext = os.path.splitext(item)[1].lower()\n",
    "            if file_ext in extensions_lower:\n",
    "                file_list.append(item_full_path)\n",
    "    \n",
    "    return file_list\n",
    "\n",
    "file_path = \"./pdfs\"\n",
    "files = list_all_files_basic(file_path)\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2a558b5-80c7-45e4-b082-7b8e5fdd7da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_categories(file_path, full_text):\n",
    "    prompt = f\"\"\"\n",
    "    Based on the following content, output only Numpy Dict with no other content:\n",
    "    {{\"categories\":[\"...\"]}}\n",
    "    \n",
    "    Content:\n",
    "    {full_text[:1000]}\n",
    "    \"\"\"\n",
    "    \n",
    "    result = ollama.chat(model=LLM_MODEL, messages=[{\"role\":\"user\",\"content\":prompt}])\n",
    "    cat_tags = ast.literal_eval(result[\"message\"][\"content\"])\n",
    "\n",
    "    categories_list[\"General Document\"] = [file_path]\n",
    "        \n",
    "    for category in cat_tags[\"categories\"]:\n",
    "        if category not in categories_list:\n",
    "            categories_list[category] = []\n",
    "        categories_list[category].append(file_path)\n",
    "    return categories_list\n",
    "\n",
    "\n",
    "# def process_single_pdf(file):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bbd2018-0291-46c1-b935-b73c4a563f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# categories_list = {}\n",
    "# for file in files:\n",
    "#     full_text = ocr_single_pdf(file)\n",
    "#     categories_list.update(classify_categories(file, full_text))\n",
    "# categories_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "befe8bb4-a7bf-432c-9536-376f61e47d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_chunks(text, chunk_size=500, chunk_overlap=50):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    text_length = len(text)\n",
    "    # Priority separators (semantic order: paragraph > sentence > punctuation)\n",
    "    separators = [\"\\n\\n\", \"\\n\", \"ã€‚\", \"ï¼\", \"ï¼Ÿ\", \"ï¼›\", \"ï¼Œ\", \"ã€\", \".\", \"!\", \"?\", \";\", \",\"]\n",
    "\n",
    "    while start < text_length:\n",
    "        end = start + chunk_size\n",
    "        # Add remaining text as last chunk if end exceeds text length\n",
    "        if end >= text_length:\n",
    "            chunks.append(text[start:].strip())\n",
    "            break\n",
    "        \n",
    "        temp_chunk = text[start:end]\n",
    "        split_pos = -1\n",
    "        # Find last valid separator (after overlap threshold)\n",
    "        for sep in separators:\n",
    "            pos = temp_chunk.rfind(sep)\n",
    "            if pos != -1 and pos > (chunk_size - chunk_overlap):\n",
    "                split_pos = pos\n",
    "                break\n",
    "        \n",
    "        # Split at natural separator if found\n",
    "        if split_pos != -1:\n",
    "            chunk_end = start + split_pos + 1\n",
    "            chunks.append(text[start:chunk_end].strip())\n",
    "            start = chunk_end - chunk_overlap\n",
    "        # Fallback: split at chunk size with overlap\n",
    "        else:\n",
    "            chunks.append(temp_chunk.strip())\n",
    "            start = end - chunk_overlap\n",
    "\n",
    "    # Filter out empty chunks\n",
    "    return [c for c in chunks if c]\n",
    "\n",
    "# chunks = split_text_chunks(full_text)\n",
    "# chunks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c3db8be-e754-47e5-9302-7b1fcb44c370",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_chunk(chunk):\n",
    "    try:\n",
    "        # Call Ollama API to get embedding for the chunk\n",
    "        res = ollama.embed(model=EMBEDDING_MODEL, input=chunk)\n",
    "        # Convert embedding to float32 numpy array for efficiency\n",
    "        return np.array(res[\"embeddings\"], dtype=np.float32)\n",
    "    except:\n",
    "        # Return empty array if embedding generation fails\n",
    "        return np.array([])\n",
    "\n",
    "def parallel_get_embeddings(chunks):\n",
    "    all_emb = []\n",
    "    # Use process pool for parallel embedding (faster for CPU/GPU-bound tasks)\n",
    "    with ProcessPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        # Submit embedding tasks for all chunks\n",
    "        futs = [executor.submit(embed_chunk, b) for b in chunks]\n",
    "        # Collect valid embedding results\n",
    "        for f in futs:\n",
    "            be = f.result()\n",
    "            if len(be) > 0:\n",
    "                all_emb.append(be[0])\n",
    "                \n",
    "    if all_emb:\n",
    "        final = np.vstack(all_emb)\n",
    "        print(f\"âœ… Finished Embedding, Dims: {final.shape}\")\n",
    "        return final\n",
    "    # Return empty array if no valid embeddings\n",
    "    return np.array([])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea2c0d90-922d-48b7-969b-c21129fe5b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_two_dict_lists(dict1, dict2):\n",
    "    merged_dict = {}\n",
    "    for key, value in dict1.items():\n",
    "        merged_dict[key] = value.copy() if isinstance(value, list) else [value]\n",
    "    \n",
    "    for key, value in dict2.items():\n",
    "        list_value = value.copy() if isinstance(value, list) else [value]\n",
    "        \n",
    "        if key in merged_dict:\n",
    "            merged_dict[key] += list_value\n",
    "        else:\n",
    "            merged_dict[key] = list_value\n",
    "    \n",
    "    return merged_dict\n",
    "\n",
    "def classify_text_chunk(chunk):    \n",
    "    chunk_lower = chunk.lower()\n",
    "    category_scores = {cat: 0 for cat in categories_list}\n",
    "\n",
    "    for category in categories_list:\n",
    "        category_scores[category] = chunk_lower.count(category.lower())\n",
    "    \n",
    "    max_score = max(category_scores.values())\n",
    "    return max(category_scores, key=category_scores.get) if max_score > 0 else \"General Document\"\n",
    "    \n",
    "def classify_chunks(text):\n",
    "    return [classify_text_chunk(c) for c in text]\n",
    "\n",
    "# question=\"What is FPGA?\"\n",
    "# keyword = classify_text_chunk(question)\n",
    "# print(keyword)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef68fa59-0e38-4122-9aeb-24956abee4f5",
   "metadata": {},
   "source": [
    "## å®Œæ•´ç‰ˆ\n",
    "é¢„å¤„ç†ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77528711-6884-4691-bd5d-d8582c27ad6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš€ GLM-OCR processing: Ubuntuå¦‚ä½•ç›´æ¥æ‰‹åŠ¨æ§åˆ¶é£æ‰‡é€Ÿåº¦.pdf\n",
      "âœ… OCR completed! Text length: 3681 characters\n",
      "âœ… Finished Embedding, Dims: (3, 768)\n",
      "\n",
      "ğŸš€ GLM-OCR processing: Guide of developing Tang Nano FPGA on Mac.pdf\n",
      "âœ… OCR completed! Text length: 13816 characters\n",
      "âœ… Finished Embedding, Dims: (24, 768)\n"
     ]
    }
   ],
   "source": [
    "full_embeddings = {}\n",
    "categories_list = {}\n",
    "categories_list[\"General Document\"] = []\n",
    "\n",
    "def get_all_pdfs_embeddings(pdf_path):\n",
    "    global categories_list\n",
    "    \n",
    "    files = list_all_files_basic(file_path)\n",
    "    for file in files:\n",
    "        full_text = ocr_single_pdf(file)\n",
    "        categories_list = merge_two_dict_lists(categories_list, classify_categories(file, full_text))\n",
    "        chunks = split_text_chunks(full_text)\n",
    "        embeddings = parallel_get_embeddings(chunks)\n",
    "        full_embeddings[file] = embeddings\n",
    "        \n",
    "get_all_pdfs_embeddings(\"./pdfs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1944a97-366c-4c5d-87c5-6c547a3af9a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "General Document\n"
     ]
    }
   ],
   "source": [
    "question=\"What is FPGA?\"\n",
    "keyword = classify_text_chunk(question)\n",
    "print(keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da33337d-b996-4cea-b54a-b411c1108ca2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'General Document': ['./pdfs/Guide of developing Tang Nano FPGA on Mac.pdf',\n",
       "  './pdfs/Guide of developing Tang Nano FPGA on Mac.pdf'],\n",
       " 'Linux': ['./pdfs/Ubuntuå¦‚ä½•ç›´æ¥æ‰‹åŠ¨æ§åˆ¶é£æ‰‡é€Ÿåº¦.pdf',\n",
       "  './pdfs/Ubuntuå¦‚ä½•ç›´æ¥æ‰‹åŠ¨æ§åˆ¶é£æ‰‡é€Ÿåº¦.pdf',\n",
       "  './pdfs/Ubuntuå¦‚ä½•ç›´æ¥æ‰‹åŠ¨æ§åˆ¶é£æ‰‡é€Ÿåº¦.pdf',\n",
       "  './pdfs/Ubuntuå¦‚ä½•ç›´æ¥æ‰‹åŠ¨æ§åˆ¶é£æ‰‡é€Ÿåº¦.pdf'],\n",
       " 'Hardware': ['./pdfs/Ubuntuå¦‚ä½•ç›´æ¥æ‰‹åŠ¨æ§åˆ¶é£æ‰‡é€Ÿåº¦.pdf',\n",
       "  './pdfs/Ubuntuå¦‚ä½•ç›´æ¥æ‰‹åŠ¨æ§åˆ¶é£æ‰‡é€Ÿåº¦.pdf',\n",
       "  './pdfs/Guide of developing Tang Nano FPGA on Mac.pdf',\n",
       "  './pdfs/Ubuntuå¦‚ä½•ç›´æ¥æ‰‹åŠ¨æ§åˆ¶é£æ‰‡é€Ÿåº¦.pdf',\n",
       "  './pdfs/Ubuntuå¦‚ä½•ç›´æ¥æ‰‹åŠ¨æ§åˆ¶é£æ‰‡é€Ÿåº¦.pdf',\n",
       "  './pdfs/Guide of developing Tang Nano FPGA on Mac.pdf'],\n",
       " 'Sysfs': ['./pdfs/Ubuntuå¦‚ä½•ç›´æ¥æ‰‹åŠ¨æ§åˆ¶é£æ‰‡é€Ÿåº¦.pdf',\n",
       "  './pdfs/Ubuntuå¦‚ä½•ç›´æ¥æ‰‹åŠ¨æ§åˆ¶é£æ‰‡é€Ÿåº¦.pdf',\n",
       "  './pdfs/Ubuntuå¦‚ä½•ç›´æ¥æ‰‹åŠ¨æ§åˆ¶é£æ‰‡é€Ÿåº¦.pdf',\n",
       "  './pdfs/Ubuntuå¦‚ä½•ç›´æ¥æ‰‹åŠ¨æ§åˆ¶é£æ‰‡é€Ÿåº¦.pdf'],\n",
       " 'Fan Control': ['./pdfs/Ubuntuå¦‚ä½•ç›´æ¥æ‰‹åŠ¨æ§åˆ¶é£æ‰‡é€Ÿåº¦.pdf',\n",
       "  './pdfs/Ubuntuå¦‚ä½•ç›´æ¥æ‰‹åŠ¨æ§åˆ¶é£æ‰‡é€Ÿåº¦.pdf',\n",
       "  './pdfs/Ubuntuå¦‚ä½•ç›´æ¥æ‰‹åŠ¨æ§åˆ¶é£æ‰‡é€Ÿåº¦.pdf',\n",
       "  './pdfs/Ubuntuå¦‚ä½•ç›´æ¥æ‰‹åŠ¨æ§åˆ¶é£æ‰‡é€Ÿåº¦.pdf'],\n",
       " 'PWM': ['./pdfs/Ubuntuå¦‚ä½•ç›´æ¥æ‰‹åŠ¨æ§åˆ¶é£æ‰‡é€Ÿåº¦.pdf',\n",
       "  './pdfs/Ubuntuå¦‚ä½•ç›´æ¥æ‰‹åŠ¨æ§åˆ¶é£æ‰‡é€Ÿåº¦.pdf',\n",
       "  './pdfs/Ubuntuå¦‚ä½•ç›´æ¥æ‰‹åŠ¨æ§åˆ¶é£æ‰‡é€Ÿåº¦.pdf',\n",
       "  './pdfs/Ubuntuå¦‚ä½•ç›´æ¥æ‰‹åŠ¨æ§åˆ¶é£æ‰‡é€Ÿåº¦.pdf'],\n",
       " 'Dell': ['./pdfs/Ubuntuå¦‚ä½•ç›´æ¥æ‰‹åŠ¨æ§åˆ¶é£æ‰‡é€Ÿåº¦.pdf',\n",
       "  './pdfs/Ubuntuå¦‚ä½•ç›´æ¥æ‰‹åŠ¨æ§åˆ¶é£æ‰‡é€Ÿåº¦.pdf',\n",
       "  './pdfs/Ubuntuå¦‚ä½•ç›´æ¥æ‰‹åŠ¨æ§åˆ¶é£æ‰‡é€Ÿåº¦.pdf',\n",
       "  './pdfs/Ubuntuå¦‚ä½•ç›´æ¥æ‰‹åŠ¨æ§åˆ¶é£æ‰‡é€Ÿåº¦.pdf'],\n",
       " 'T5810': ['./pdfs/Ubuntuå¦‚ä½•ç›´æ¥æ‰‹åŠ¨æ§åˆ¶é£æ‰‡é€Ÿåº¦.pdf',\n",
       "  './pdfs/Ubuntuå¦‚ä½•ç›´æ¥æ‰‹åŠ¨æ§åˆ¶é£æ‰‡é€Ÿåº¦.pdf',\n",
       "  './pdfs/Ubuntuå¦‚ä½•ç›´æ¥æ‰‹åŠ¨æ§åˆ¶é£æ‰‡é€Ÿåº¦.pdf',\n",
       "  './pdfs/Ubuntuå¦‚ä½•ç›´æ¥æ‰‹åŠ¨æ§åˆ¶é£æ‰‡é€Ÿåº¦.pdf'],\n",
       " 'FPGA Development': ['./pdfs/Guide of developing Tang Nano FPGA on Mac.pdf',\n",
       "  './pdfs/Guide of developing Tang Nano FPGA on Mac.pdf'],\n",
       " 'IDE Alternatives': ['./pdfs/Guide of developing Tang Nano FPGA on Mac.pdf',\n",
       "  './pdfs/Guide of developing Tang Nano FPGA on Mac.pdf'],\n",
       " 'Open Source Toolchain': ['./pdfs/Guide of developing Tang Nano FPGA on Mac.pdf',\n",
       "  './pdfs/Guide of developing Tang Nano FPGA on Mac.pdf'],\n",
       " 'Learning Resources': ['./pdfs/Guide of developing Tang Nano FPGA on Mac.pdf',\n",
       "  './pdfs/Guide of developing Tang Nano FPGA on Mac.pdf'],\n",
       " 'Licensing': ['./pdfs/Guide of developing Tang Nano FPGA on Mac.pdf',\n",
       "  './pdfs/Guide of developing Tang Nano FPGA on Mac.pdf'],\n",
       " 'Design Flow': ['./pdfs/Guide of developing Tang Nano FPGA on Mac.pdf',\n",
       "  './pdfs/Guide of developing Tang Nano FPGA on Mac.pdf'],\n",
       " 'Compilation': ['./pdfs/Guide of developing Tang Nano FPGA on Mac.pdf',\n",
       "  './pdfs/Guide of developing Tang Nano FPGA on Mac.pdf'],\n",
       " 'Hardware Description Languages': ['./pdfs/Guide of developing Tang Nano FPGA on Mac.pdf',\n",
       "  './pdfs/Guide of developing Tang Nano FPGA on Mac.pdf'],\n",
       " 'Development Board': ['./pdfs/Guide of developing Tang Nano FPGA on Mac.pdf',\n",
       "  './pdfs/Guide of developing Tang Nano FPGA on Mac.pdf'],\n",
       " 'Vendor': ['./pdfs/Guide of developing Tang Nano FPGA on Mac.pdf',\n",
       "  './pdfs/Guide of developing Tang Nano FPGA on Mac.pdf'],\n",
       " 'Workflow': ['./pdfs/Guide of developing Tang Nano FPGA on Mac.pdf',\n",
       "  './pdfs/Guide of developing Tang Nano FPGA on Mac.pdf']}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea3fa9f-3653-4856-989d-28a543acec1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {'General Document': ['./pdfs/Ubuntuå¦‚ä½•ç›´æ¥æ‰‹åŠ¨æ§åˆ¶é£æ‰‡é€Ÿåº¦.pdf',],}\n",
    "print(a)\n",
    "a['General Document'].append(\"12\")\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0197e0-c4be-404f-92c2-8c90bf55179e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d877ee24-6cbe-452e-8729-22301d9af19b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
