{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2db84a35-8914-429e-ac16-6557ec9a03c2",
   "metadata": {},
   "source": [
    "åœ¨ä¸Šä¸€ç¯‡ä¸­ï¼Œæˆ‘ä»¬æ‰‹åŠ¨å®Œæˆäº† RAG çš„å·¥ä½œæµç¨‹ï¼Œä½†æ˜¯å®é™…ä½¿ç”¨å½“ä¸­ï¼Œæˆ‘ä»¬éœ€è¦å®ƒè‡ªåŠ¨è¿›è¡Œã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†ä¿®æ”¹ä»£ç ï¼Œè®©è¿™ä¸ªå·¥ä½œæµç¨‹è‡ªåŠ¨åŒ–è¿è¡Œã€‚\n",
    "\n",
    "é¦–å…ˆæˆ‘ä»¬åˆ—å‡ºæ‰€æœ‰çš„é¢„è®¾å’Œå‡½æ•°å£°æ˜ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c301aad1-371e-4599-8cf2-da3ad257da10",
   "metadata": {},
   "source": [
    "### Global Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc09da17-902d-4e3b-998c-33d3fe0687bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import gc\n",
    "import ast\n",
    "import time\n",
    "import base64\n",
    "import ollama\n",
    "import numpy as np\n",
    "from io import BytesIO\n",
    "from pdf2image import convert_from_path\n",
    "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed\n",
    "\n",
    "\n",
    "# ====================== Model Configuration ======================\n",
    "OCR_MODEL = \"glm-ocr\"\n",
    "OCR_DPI = 100\n",
    "OCR_PROMPT = \"Text recognition. Accurately extract all text, LaTeX formulas, and symbols from the image. Output only the original content.\"\n",
    "OLLAMA_HOST = \"http://localhost:11434\"\n",
    "\n",
    "LLM_MODEL = \"gpt-oss:20b\"\n",
    "\n",
    "EMBEDDING_MODEL = \"embeddinggemma\"\n",
    "\n",
    "MAX_WORKERS = 8  # The Threads of Embedding. Suit your CPU. Higher is better.\n",
    "\n",
    "# ====================== Global Variables ======================\n",
    "FILES_PATH = \"./pdfs\"\n",
    "\n",
    "full_embeddings = {}\n",
    "categories_list = {}\n",
    "full_chunks = {}\n",
    "categories_list[\"General Document\"] = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f041497-9238-44f9-8bad-666c5bba3a49",
   "metadata": {},
   "source": [
    "### OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5138808a-8c22-4cdc-84be-6beff5abb65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================== Image to Base64 ======================\n",
    "def image_to_base64(image):\n",
    "    buffered = BytesIO()\n",
    "    image.save(buffered, format=\"PNG\", quality=80, optimize=True)\n",
    "    return base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "# ====================== OCR single page ======================\n",
    "def ocr_single_page(page_img):\n",
    "    img_base64 = image_to_base64(page_img)\n",
    "    response = ollama.chat(\n",
    "        model=OCR_MODEL,\n",
    "        messages=[{'role': 'user', 'content': OCR_PROMPT, 'images': [img_base64]}],\n",
    "    )\n",
    "    return response[\"message\"][\"content\"].strip()\n",
    "\n",
    "def ocr_single_pdf(pdf_path):\n",
    "    full_text = \"\"\n",
    "    try:\n",
    "        # Get PDF file name\n",
    "        pdf_file_name = os.path.basename(pdf_path)\n",
    "        \n",
    "        print(f\"\\nğŸš€ GLM-OCR processing: {pdf_file_name}\")\n",
    "        \n",
    "        # Convert PDF to images\n",
    "        pages = convert_from_path(\n",
    "            pdf_path, dpi=OCR_DPI, thread_count=1, use_pdftocairo=True, grayscale=True\n",
    "        )\n",
    "        total_pages = len(pages)\n",
    "    \n",
    "        # Iterate through each page to process OCR\n",
    "        for idx, page_img in enumerate(pages):\n",
    "            page_text = \"\"\n",
    "            # Retry mechanism (max 2 attempts)\n",
    "            for retry in range(2):\n",
    "                try:\n",
    "                    # OCR \n",
    "                    page_text = ocr_single_page(page_img)\n",
    "                    break  # Exit retry if successful\n",
    "                except TimeoutError:\n",
    "                    print(f\"   âš ï¸ Page {idx+1} OCR timed out, retrying {retry+1}...\")\n",
    "                    time.sleep(2)\n",
    "                except Exception as e:\n",
    "                    print(f\"   âš ï¸ Page {idx+1} OCR failed: {e}\")\n",
    "                    time.sleep(2)\n",
    "            \n",
    "            # Fill placeholder text when OCR fails\n",
    "            if not page_text:\n",
    "                page_text = f\"[Recognition failed for Page {idx+1}]\"\n",
    "            \n",
    "            # Concatenate text\n",
    "            full_text += page_text + \"\\n\\n\"\n",
    "            \n",
    "    \n",
    "        # Clean up redundant whitespace characters\n",
    "        full_text = re.sub(r'\\s+', ' ', full_text).strip()\n",
    "        print(f\"âœ… OCR completed! Text length: {len(full_text)} characters\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ OCR failed: {str(e)}\")\n",
    "        full_text = \"\"\n",
    "\n",
    "    clean_text = full_text.replace('\"', '').replace(\"'\", \"\").replace(\"[\", \"\").replace(\"]\", \"\").replace(\"{\", \"\").replace(\"}\", \"\")\n",
    "\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c844b638-24f4-4639-ba62-477080851c17",
   "metadata": {},
   "source": [
    "### List all files in a directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5eee84f5-f838-42b9-9566-32bee5e45edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_all_files_in_dir(path):\n",
    "    extensions=['.pdf']\n",
    "    \n",
    "    if not os.path.exists(path):\n",
    "        print(f\"é”™è¯¯ï¼šè·¯å¾„ '{path}' ä¸å­˜åœ¨\")\n",
    "        return []\n",
    "    \n",
    "    extensions_lower = [ext.lower() for ext in extensions]\n",
    "    file_list = []\n",
    "    \n",
    "    for item in os.listdir(path):\n",
    "        item_full_path = os.path.join(path, item)\n",
    "        if os.path.isfile(item_full_path):\n",
    "            file_ext = os.path.splitext(item)[1].lower()\n",
    "            if file_ext in extensions_lower:\n",
    "                file_list.append(item_full_path)\n",
    "    \n",
    "    return file_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cad3321-0ca9-496c-a425-99abbaa3746a",
   "metadata": {},
   "source": [
    "### Classify Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2a558b5-80c7-45e4-b082-7b8e5fdd7da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_categories(file_path, full_text):\n",
    "    prompt = f\"\"\"\n",
    "Based on the following content, output only Numpy Dict with no other content:\n",
    "{{\"categories\":[\"...\"]}}\n",
    "    \n",
    "Content:\n",
    "{full_text[:1000]}\n",
    "\"\"\"\n",
    "    \n",
    "    result = ollama.chat(model=LLM_MODEL, messages=[{\"role\":\"user\",\"content\":prompt}])\n",
    "    cat_tags = ast.literal_eval(result[\"message\"][\"content\"])\n",
    "\n",
    "    categories_list[\"General Document\"] = [file_path]\n",
    "        \n",
    "    for category in cat_tags[\"categories\"]:\n",
    "        if category not in categories_list:\n",
    "            categories_list[category] = []\n",
    "        categories_list[category].append(file_path)\n",
    "    return categories_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060656de-dcb6-4a7d-a03e-97b64d2604d7",
   "metadata": {},
   "source": [
    "### Split Text to Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "befe8bb4-a7bf-432c-9536-376f61e47d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_chunks(text, chunk_size=500, chunk_overlap=50):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    text_length = len(text)\n",
    "    # Priority separators (semantic order: paragraph > sentence > punctuation)\n",
    "    separators = [\"\\n\\n\", \"\\n\", \"\\'\", \"ã€‚\", \"ï¼\", \"ï¼Ÿ\", \"ï¼›\", \"ï¼Œ\", \"ã€\", \"ï¼š\", \".\", \"!\", \"?\", \";\", \",\"]\n",
    "\n",
    "    while start < text_length:\n",
    "        end = start + chunk_size\n",
    "        # Add remaining text as last chunk if end exceeds text length\n",
    "        if end >= text_length:\n",
    "            chunks.append(text[start:].strip())\n",
    "            break\n",
    "        \n",
    "        temp_chunk = text[start:end]\n",
    "        split_pos = -1\n",
    "        # Find last valid separator (after overlap threshold)\n",
    "        for sep in separators:\n",
    "            pos = temp_chunk.rfind(sep)\n",
    "            if pos != -1 and pos > (chunk_size - chunk_overlap):\n",
    "                split_pos = pos\n",
    "                break\n",
    "        \n",
    "        # Split at natural separator if found\n",
    "        if split_pos != -1:\n",
    "            chunk_end = start + split_pos + 1\n",
    "            chunks.append(text[start:chunk_end].strip())\n",
    "            start = chunk_end - chunk_overlap\n",
    "        # Fallback: split at chunk size with overlap\n",
    "        else:\n",
    "            chunks.append(temp_chunk.strip())\n",
    "            start = end - chunk_overlap\n",
    "\n",
    "    # Filter out empty chunks\n",
    "    return [c for c in chunks if c]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec24e43-089a-400f-96c6-56309efe5eeb",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ba1bedb-1e3b-4978-ad6a-45c337a2baed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_chunk(chunk):\n",
    "    try:\n",
    "        #  embedding chunk\n",
    "        res = ollama.embed(model=EMBEDDING_MODEL, input=chunk)\n",
    "        # Convert embedding to float32 numpy array for efficiency\n",
    "        return np.array(res[\"embeddings\"], dtype=np.float32)\n",
    "    except:\n",
    "        # Return empty array if embedding generation fails\n",
    "        return np.array([])\n",
    "\n",
    "def parallel_get_embeddings(chunks):\n",
    "    all_emb = []\n",
    "    # Use process pool for parallel embedding (faster for CPU/GPU-bound tasks)\n",
    "    with ProcessPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        # Submit embedding tasks for all chunks\n",
    "        futs = [executor.submit(embed_chunk, b) for b in chunks]\n",
    "        # Collect valid embedding results\n",
    "        for f in futs:\n",
    "            be = f.result()\n",
    "            if len(be) > 0:\n",
    "                all_emb.append(be[0])\n",
    "                \n",
    "    if all_emb:\n",
    "        final = np.vstack(all_emb)\n",
    "        print(f\"âœ… Finished Embedding\")\n",
    "        return final\n",
    "    # Return empty array if no valid embeddings\n",
    "    return np.array([])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f116a6cf-efd3-4bbf-90d3-472e5d9d9ec9",
   "metadata": {},
   "source": [
    "### Some function for usages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f5216c3-d1db-42d0-a66f-d895463886a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_two_dict_lists(dict1, dict2):\n",
    "    merged_dict = {}\n",
    "    for key, value in dict1.items():\n",
    "        merged_dict[key] = value.copy() if isinstance(value, list) else [value]\n",
    "    \n",
    "    for key, value in dict2.items():\n",
    "        list_value = value.copy() if isinstance(value, list) else [value]\n",
    "        \n",
    "        if key in merged_dict:\n",
    "            merged_dict[key] += list_value\n",
    "        else:\n",
    "            merged_dict[key] = list_value\n",
    "    \n",
    "    return merged_dict\n",
    "    \n",
    "def cosine_similarity(vec1, vec2):\n",
    "    n1 = np.linalg.norm(vec1)\n",
    "    n2 = np.linalg.norm(vec2)\n",
    "    \n",
    "    if n1 == 0 or n2 == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    return np.dot(vec1/n1, vec2/n2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e322fc-c618-4e76-ae3c-aa49571ce81a",
   "metadata": {},
   "source": [
    "### Find best Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7622b217-0316-43f8-b4ae-ae92bd53b7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_category(question):\n",
    "    max_similarity = -1.0  # Track highest similarity score\n",
    "    best_category = \"\"     # Track category with highest similarity\n",
    "    question_embedding = ollama.embed(model=EMBEDDING_MODEL, input=question)[\"embeddings\"][0]\n",
    "    # Iterate through all categories to find most similar one\n",
    "    for category in categories_list:\n",
    "        # Generate embedding for current category and calculate similarity to question\n",
    "        similarity = cosine_similarity(question_embedding, ollama.embed(model=EMBEDDING_MODEL, input=category)[\"embeddings\"][0])\n",
    "        \n",
    "        # Print category (left-aligned) and similarity (12 decimal places)\n",
    "        print(f\"{category:<{20}} {similarity:.{12}f}\")\n",
    "        \n",
    "        # Update top category if current similarity is higher\n",
    "        if similarity > max_similarity:\n",
    "            max_similarity = similarity\n",
    "            best_category = category\n",
    "    \n",
    "    return best_category"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff091b2-50bf-42cf-a7c8-8588163c7783",
   "metadata": {},
   "source": [
    "### Get relevant chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "610a2d39-242c-4dbd-8a69-d6fd9efa6502",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_chunks(question):\n",
    "    global full_embeddings, full_chunks\n",
    "    results = []\n",
    "    question_embedding = ollama.embed(model=EMBEDDING_MODEL, input=question)[\"embeddings\"][0]\n",
    "    for file in full_embeddings:\n",
    "        index = 0\n",
    "        files = list_all_files_in_dir(FILES_PATH)\n",
    "        for file in files:\n",
    "            for idx, embedding in enumerate(full_embeddings[file]):\n",
    "                similarity = cosine_similarity(question_embedding, embedding)\n",
    "                results.append((idx, similarity, file, full_chunks[file][idx]))\n",
    "    \n",
    "    seen = set()\n",
    "    unique_results = []\n",
    "    for item in results:\n",
    "        if item not in seen:\n",
    "            seen.add(item)\n",
    "            unique_results.append(item)\n",
    "    \n",
    "    unique_results.sort(key=lambda x: x[1], reverse=True)\n",
    "    return unique_results[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef68fa59-0e38-4122-9aeb-24956abee4f5",
   "metadata": {},
   "source": [
    "## Work Flow\n",
    "### Process: Prepare embeddings, categories and anything we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77528711-6884-4691-bd5d-d8582c27ad6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš€ GLM-OCR processing: æ‘„å½±æœ¯è¯­ï¼šlogã€åŠ¨æ€èŒƒå›´å’Œè‰²å½©å–æ ·.pdf\n",
      "âœ… OCR completed! Text length: 5996 characters\n",
      "âœ… Finished Embedding\n",
      "\n",
      "ğŸš€ GLM-OCR processing: What is the differences between gcc and Clang.pdf\n",
      "âœ… OCR completed! Text length: 15804 characters\n",
      "âœ… Finished Embedding\n",
      "\n",
      "ğŸš€ GLM-OCR processing: How to Adjust NVIDIA GPU Frequency, Volte and Other Settings on Linux and Overlocking.pdf\n",
      "âœ… OCR completed! Text length: 14016 characters\n",
      "âœ… Finished Embedding\n",
      "\n",
      "ğŸš€ GLM-OCR processing: Use gcc to learn C compilation process.pdf\n",
      "   âš ï¸ Page 9 OCR failed: an error was encountered while running the model: GGML_ASSERT(a->ne[2] * 4 == b->ne[0]) failed (status code: 500)\n",
      "   âš ï¸ Page 9 OCR failed: an error was encountered while running the model: GGML_ASSERT(a->ne[2] * 4 == b->ne[0]) failed (status code: 500)\n",
      "âœ… OCR completed! Text length: 13109 characters\n",
      "âœ… Finished Embedding\n",
      "\n",
      "ğŸš€ GLM-OCR processing: Maximum Likelihood Estimation with Code.pdf\n",
      "âœ… OCR completed! Text length: 18227 characters\n",
      "âœ… Finished Embedding\n",
      "\n",
      "ğŸš€ GLM-OCR processing: Ubuntuå¦‚ä½•ç›´æ¥æ‰‹åŠ¨æ§åˆ¶é£æ‰‡é€Ÿåº¦.pdf\n",
      "âœ… OCR completed! Text length: 3681 characters\n",
      "âœ… Finished Embedding\n",
      "\n",
      "ğŸš€ GLM-OCR processing: Guide of developing Tang Nano FPGA on Mac.pdf\n",
      "âœ… OCR completed! Text length: 13816 characters\n",
      "âœ… Finished Embedding\n",
      "\n",
      "ğŸš€ GLM-OCR processing: Animagine XL V3.1 Complete Guide.pdf\n",
      "âœ… OCR completed! Text length: 7910 characters\n",
      "âœ… Finished Embedding\n"
     ]
    }
   ],
   "source": [
    "def get_all_pdfs_embeddings(pdf_path):\n",
    "    global categories_list, full_embeddings, full_chunks\n",
    "    \n",
    "    files = list_all_files_in_dir(pdf_path)\n",
    "    for file in files:\n",
    "        full_text = ocr_single_pdf(file)\n",
    "        categories_list = merge_two_dict_lists(categories_list, classify_categories(file, full_text))\n",
    "        chunks = split_text_chunks(full_text)\n",
    "        full_chunks[file] = chunks\n",
    "        embeddings = parallel_get_embeddings(chunks)\n",
    "        full_embeddings[file] = embeddings\n",
    "        \n",
    "get_all_pdfs_embeddings(FILES_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc12b285-03c9-4efa-9289-8b3d3570be58",
   "metadata": {},
   "source": [
    "### Ask questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ea3fa9f-3653-4856-989d-28a543acec1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**åœ¨ Ubuntu ä¸Šæ‰‹åŠ¨æ§åˆ¶é£æ‰‡è½¬é€Ÿï¼ˆå¸¸è§æ–¹æ³•ï¼‰**\n",
      "\n",
      "> ä¸‹é¢çš„æ­¥éª¤å…¼é¡¾ä¸€èˆ¬æ¡Œé¢ä¸»æ¿é£æ‰‡ï¼ˆéœ€è¦ `lmâ€‘sensors` ä¸ `fancontrol`ï¼‰ä»¥åŠ NVIDIA GPU çš„é£æ‰‡ã€‚  \n",
      "> å¦‚æœä½ åªæƒ³è°ƒ GPU é£æ‰‡ï¼Œåé¢â€œGPU é£æ‰‡â€éƒ¨åˆ†å³å¯ï¼›è‹¥æ˜¯æ•´ä¸ªæœºç®±é£æ‰‡ï¼Œè¯·æŒ‰â€œä¸»æ¿é£æ‰‡â€éƒ¨åˆ†æ“ä½œã€‚\n",
      "\n",
      "---\n",
      "\n",
      "## 1. ä¸»æ¿/æœºç®±é£æ‰‡ï¼ˆPWM æ–¹å¼ï¼‰\n",
      "\n",
      "| æ­¥éª¤ | å‘½ä»¤ / æ“ä½œ | è¯´æ˜ |\n",
      "|------|-------------|------|\n",
      "| 1 | `sudo apt update && sudo apt install lm-sensors fancontrol` | å®‰è£…æ„Ÿåº”ä¸æ§åˆ¶å·¥å…· |\n",
      "| 2 | `sudo sensors-detect` å¹¶æŒ‰æç¤º `yes` | è¯†åˆ«ç¡¬ä»¶ä¼ æ„Ÿå™¨ä¸ PWM èŠ¯ç‰‡ |\n",
      "| 3 | `sudo pwmconfig` | è‡ªåŠ¨ç”Ÿæˆ `/etc/fancontrol` å¹¶æµ‹è¯• PWMï¼ˆæŒ‰æç¤ºé€‰æ‹© 0â€“255ï¼‰ |\n",
      "| 4 | æ‰“å¼€ `/etc/fancontrol`ï¼Œç¼–è¾‘ `MINSTART`, `MINSTOP`, `MAXSTART`, `MAXSTOP` ç­‰å­—æ®µ | è¿™äº›å€¼å†³å®šé£æ‰‡æœ€ä½/æœ€é«˜è½¬é€Ÿä¸æ¸©åº¦é˜ˆå€¼ |\n",
      "| 5 | å¯åŠ¨æœåŠ¡ï¼š`sudo systemctl enable --now fancontrol` | è®© `fancontrol` è‡ªåŠ¨åœ¨å¯åŠ¨æ—¶è¿è¡Œ |\n",
      "| 6 | æ£€æŸ¥ï¼š`cat /sys/class/hwmon/hwmon*/pwm1` æˆ– `pwm2` | æŸ¥çœ‹å½“å‰ PWM æ•°å€¼ï¼ˆ0â€“255ï¼‰ |\n",
      "| 7 | æ‰‹åŠ¨ä¿®æ”¹ï¼š`sudo sh -c 'echo 180 > /sys/class/hwmon/hwmon*/pwm1'` | 180 å¯¹åº”çº¦ 70% é£æ‰‡è½¬é€Ÿï¼ˆå–å†³äºèŠ¯ç‰‡ï¼‰ |\n",
      "\n",
      "> **å¸¸è§æ•°å­—**  \n",
      "> - 0 â†’ å…³é—­æˆ–æœ€ä½è½¬é€Ÿ  \n",
      "> - 255 â†’ æœ€å¤§è½¬é€Ÿ  \n",
      "> - 120â€“180 â†’ çº¦ 40â€“70%ï¼ˆæ ¹æ®ä¸»æ¿æ‰‹å†Œæ ¡å‡†ï¼‰  \n",
      "\n",
      "---\n",
      "\n",
      "## 2. NVIDIA GPU é£æ‰‡ï¼ˆ`nvidia-settings`ï¼‰\n",
      "\n",
      "> éœ€è¦ NVIDIA é©±åŠ¨å·²æ­£ç¡®å®‰è£…ä¸” `nvidia-settings` å¯æ‰§è¡Œã€‚\n",
      "\n",
      "| æ­¥éª¤ | å‘½ä»¤ | è¯´æ˜ |\n",
      "|------|------|------|\n",
      "| 1 | `nvidia-settings` | ä»¥å›¾å½¢æ–¹å¼æ‰“å¼€æˆ– `nvidia-settings --query-gpu` æŸ¥çœ‹å½“å‰çŠ¶æ€ |\n",
      "| 2 | å¼€å¯æ‰‹åŠ¨æ¨¡å¼ï¼š`nvidia-settings -a \"[gpu:0]/GPUFanControlState=1\"` | 0 ä¸ºè‡ªåŠ¨ï¼Œ1 ä¸ºæ‰‹åŠ¨ |\n",
      "| 3 | è®¾ç½®ç›®æ ‡è½¬é€Ÿï¼ˆç™¾åˆ†æ¯”ï¼‰ï¼š`nvidia-settings -a \"[fan:0]/GPUTargetFanSpeed=70\"` | 70 ä»£è¡¨ 70%ï¼ˆ0â€“100ï¼‰ |\n",
      "| 4 | ç»„åˆä¸€æ¬¡æ€§æ‰§è¡Œï¼š`nvidia-settings -a \"[gpu:0]/GPUFanControlState=1\" -a \"[fan:0]/GPUTargetFanSpeed=70\"` | ä¸€è¡Œå‘½ä»¤å³å¯å®Œæˆ |\n",
      "| 5 | æŒä¹…åŒ–ï¼šåœ¨ `~/.config/` ä¸‹çš„ `nvidia-settings-*.conf` æˆ–é€šè¿‡ `~/.nvidia-settings-rc` å†™å…¥é…ç½® | ä¾‹å¦‚ï¼š`[gpu:0]` é‡ŒåŠ  `GPUFanControlState=1` ä¸ `GPUTargetFanSpeed=70` |\n",
      "| 6 | éªŒè¯ï¼š`nvidia-settings -q GPUTargetFanSpeed` | æŸ¥çœ‹å®é™…è®¾ç½® |\n",
      "\n",
      "> **æ³¨æ„äº‹é¡¹**  \n",
      "> - å¹¶éæ‰€æœ‰ GPU éƒ½æ”¯æŒæ‰‹åŠ¨é£æ‰‡æ§åˆ¶ã€‚  \n",
      "> - è¿‡ä½æˆ–è¿‡é«˜çš„é£æ‰‡è½¬é€Ÿå¯èƒ½å¯¼è‡´æ¸©åº¦å¤±æ§æˆ–å™ªéŸ³è¿‡å¤§ã€‚  \n",
      "> - åœ¨æŸäº› Linux å‘è¡Œç‰ˆä¸­ï¼Œ`nvidia-settings` å¯èƒ½éœ€è¦ `sudo` æƒé™ã€‚\n",
      "\n",
      "---\n",
      "\n",
      "## 3. ç»ˆææ‰‹åŠ¨æ–¹å¼ï¼ˆç›´æ¥å†™ sysfsï¼‰\n",
      "\n",
      "> å¯¹äºä¸æƒ³ä½¿ç”¨å·¥å…·æˆ–è„šæœ¬çš„é«˜çº§ç”¨æˆ·ï¼Œå¯ç›´æ¥æ“ä½œ `/sys/class/hwmon/` æˆ– `/sys/class/drm/card0/device/` ä¸‹çš„ PWM èŠ‚ç‚¹ã€‚\n",
      "\n",
      "```bash\n",
      "# ä¾‹ï¼šå†™ 200ï¼ˆçº¦ 70%ï¼‰ç»™ GPU PWM\n",
      "sudo sh -c 'echo 200 > /sys/class/hwmon/hwmon*/pwm1'\n",
      "\n",
      "# æˆ–å†™ç»™ NVIDIA GPUï¼š\n",
      "sudo sh -c 'echo 200 > /sys/class/drm/card0/device/pwm1'\n",
      "```\n",
      "\n",
      "> **è¯·å…ˆç¡®è®¤è¯¥èŠ‚ç‚¹å­˜åœ¨ä¸”å¯å†™ã€‚**  \n",
      "> ä¸æ­£ç¡®çš„å†™å…¥å¯èƒ½å¯¼è‡´ç¡¬ä»¶æŸåæˆ–ç³»ç»Ÿä¸ç¨³å®šã€‚\n",
      "\n",
      "---\n",
      "\n",
      "## 4. å¸¸è§é—®é¢˜\n",
      "\n",
      "| é—®é¢˜ | è§£ç­” |\n",
      "|------|------|\n",
      "| **é£æ‰‡ä¸å“åº”** | æ£€æŸ¥ BIOS/UEFI æ˜¯å¦ç¦ç”¨äº† PWM æˆ–æœ‰â€œè‡ªåŠ¨â€æ¨¡å¼ã€‚ |\n",
      "| **é£æ‰‡å™ªéŸ³æå¤§** | å¯èƒ½è®¾ç½®äº†è¿‡é«˜çš„ PWM å€¼ï¼ˆå¦‚ 255ï¼‰ã€‚è°ƒå› 150â€“180 è¯•è¯•ã€‚ |\n",
      "| **æ¸©åº¦ä»å‡é«˜** | æ‰‹åŠ¨é£æ‰‡æ§åˆ¶åä»å¯èƒ½æ˜¯ GPU æˆ– CPU æ¸©åº¦è¿‡é«˜ï¼Œæ£€æŸ¥æ•£çƒ­å™¨æ˜¯å¦å¹²æ‰°ã€‚ |\n",
      "| **`fancontrol` æ— æ³•å¯åŠ¨** | ç¡®è®¤ `/etc/fancontrol` é…ç½®æ— è¯¯ä¸”æ‰€æœ‰ PWM èŠ¯ç‰‡éƒ½èƒ½è®¿é—®ã€‚ |\n",
      "\n",
      "---\n",
      "\n",
      "### å°ç»“\n",
      "\n",
      "1. **ä¸»æ¿é£æ‰‡** â†’ `lm-sensors + fancontrol` â†’ `sudo pwmconfig` â†’ `/etc/fancontrol`ã€‚  \n",
      "2. **NVIDIA GPU é£æ‰‡** â†’ `nvidia-settings` â†’ `GPUFanControlState=1` + `GPUTargetFanSpeed=<0â€‘100>`ã€‚  \n",
      "3. **ç›´æ¥ sysfs å†™** â†’ `echo <PWM> > /sys/.../pwm1`ï¼ˆæ…ç”¨ï¼‰ã€‚  \n",
      "\n",
      "æ ¹æ®ä½ çš„ç¡¬ä»¶ä¸éœ€æ±‚ï¼Œä»»é€‰ä¸€ç§æ–¹å¼å³å¯åœ¨ Ubuntu ç³»ç»Ÿä¸Šæ‰‹åŠ¨æ§åˆ¶é£æ‰‡è½¬é€Ÿã€‚ç¥ä½ æˆåŠŸï¼\n"
     ]
    }
   ],
   "source": [
    "def generate_answer(question):\n",
    "    relevant_chunks = get_relevant_chunks(question)\n",
    "    # concentrate chunks together \n",
    "    ctx = \"\\n\\n\".join([f\"{item[1][2]}\" for item in enumerate(relevant_chunks)])\n",
    "    prompt = f\"\"\"Please answer truthfully based on the context, prioritize extracting specific numbers and details, say 'I don't know' if you don't know, and do not fabricate content. If you know the answer, please in ask language\n",
    "Context:\n",
    "{ctx}\n",
    "Question: {question}\"\"\"\n",
    "    try:\n",
    "        resp = ollama.chat(model=LLM_MODEL, messages=[{\"role\":\"user\",\"content\":prompt}])\n",
    "        return resp[\"message\"][\"content\"]\n",
    "    except:\n",
    "        return \"Failed to generate answer\"\n",
    "        \n",
    "question=\"å¦‚ä½•åœ¨Ubuntuç³»ç»Ÿä¸Šæ‰‹åŠ¨æ§åˆ¶é£æ‰‡è½¬é€Ÿ?\"\n",
    "answer = generate_answer(question)\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
