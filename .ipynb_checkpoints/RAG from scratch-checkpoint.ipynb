{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4298132d-f38a-47dc-acfc-56ccee158935",
   "metadata": {},
   "source": [
    "First of all, let's talk about the RAG (Retrieval-augmented generation) workflow.\n",
    "\n",
    "Maybe you see some videos or blogs explaining it, balabala. **In my words, RAG is a way that reducing the length of your input to LLM, it likes a updated search engines(if you want)**.\n",
    "\n",
    "For example, you ask the LLM Model `What is FPGA?`. \n",
    "\n",
    "If the developer train model by something about FPGA or MCU, maybe it can answer something. But if not, the model will say something randomly. So today LLM, like ChatGPT, use internet or self-dataset to search what they don't \"remember\".\n",
    "\n",
    "Here also is problem, they don't know which is right, or they misunderstand by the volumious data. For a precise and reliable answer, RAG is all your need. \n",
    "\n",
    "You just need to prepare the data you think reliable, RAG will ask LLM with related-data (not all your data). And LLM also just answer your question by your data. \n",
    "\n",
    "So to develop a RAG, we need to do :\n",
    "1. Prepare your data,\n",
    "2. Embedding your data,\n",
    "3. Finding related data,\n",
    "4. Ask LLM with related data.\n",
    "\n",
    "I will step-by-step develop a RAG system, and you will know the meanings and details of the above four steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a06e411-4a98-46f7-9cd8-427dddbd4c95",
   "metadata": {},
   "source": [
    "We will use [Ollama](https://ollama.com/), [GLM-OCR](https://ollama.com/library/glm-ocr), [embeddinggemma](https://ollama.com/library/embeddinggemma) and [GPT-OSS 20B](https://ollama.com/library/gpt-oss:20b). \n",
    ">If your device can't use gpt-oss 20b, just change it to which LLM you can run. I recommand [Gemma3 4B](https://ollama.com/library/gemma3:4b), but it maybe not work well in multi-language data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e14aec-e509-420b-81c9-a769b2cc132b",
   "metadata": {},
   "source": [
    "## Prepare data\n",
    "I prepare some PDFs in `./pdfs`. They are my blogs and notes in Chinese and English. And they are about photography, mathematic, programming and Ubuntu. You can run the below cell to see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e3e851-e558-4012-88da-86545d489c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/zhonguncle\n"
     ]
    }
   ],
   "source": [
    "!ls ./pdfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536b100d-d034-4ce5-af07-d9a9b5841bd5",
   "metadata": {},
   "source": [
    "## Embedding data\n",
    "What is embedding?\n",
    "In short, embedding is convert any data to a tensor (or vector). \n",
    "\n",
    "Why to do this? \n",
    "Now we can use mathematical methods to it, like calculating cosine distance of 2 tensor can find the similar data of specfied data. It is the core of Machine Learning: convert something to what we can do some computing on it. \n",
    "\n",
    "But the convert is not randomly, we need to make similar data closer in space. \n",
    ">If you want to know more about it, please watch [Stanford CS224N: NLP with Deep Learning | Spring 2024 | Lecture 1 - Intro and Word Vectors](https://www.youtube.com/watch?v=DzpHeXVSC5I). It show how words to a vector. Yes, you can convert image, even video to a vector. But this is not the content today.\n",
    "\n",
    "You can train a embedding model from zero, but the size of data and gpu is huge, it is not for individual. So we use the [embeddinggemma](https://ollama.com/library/embeddinggemma) to do it.\n",
    "\n",
    "This step can be devided to 5 samll steps:\n",
    "1. Get text from PDF file (you can update it to many format after learning, like EPUB);\n",
    "2. Split content to chunk,\n",
    "3. Embedding chunk,\n",
    "4. Save it (Optional),\n",
    "5. Use it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34d7896-2c9d-4a5d-b4dd-ef657c889b20",
   "metadata": {},
   "source": [
    "### Get text from PDF file\n",
    "You can extract sentences from PDF file directly, but in actual, many PDF files can't be extracted sentences directly, such as pure images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec069620-3c10-42fb-85fa-130268ceb93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import gc\n",
    "import base64\n",
    "import ollama\n",
    "from io import BytesIO\n",
    "from pdf2image import convert_from_path\n",
    "\n",
    "# ====================== OCR Model Configuration ======================\n",
    "OCR_MODEL = \"glm-ocr\"\n",
    "OCR_DPI = 100\n",
    "OCR_PROMPT = \"Text recognition. Accurately extract all text, LaTeX formulas, and symbols from the image. Output only the original content.\"\n",
    "OLLAMA_HOST = \"http://localhost:11434\"\n",
    "\n",
    "\n",
    "# ====================== Image to Base64 ======================\n",
    "def image_to_base64(image):\n",
    "    buffered = BytesIO()\n",
    "    image.save(buffered, format=\"PNG\", quality=80, optimize=True)\n",
    "    return base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "# ====================== OCR single page ======================\n",
    "def ocr_single_page(page_img, prompt):\n",
    "    img_base64 = image_to_base64(page_img)\n",
    "    response = ollama.chat(\n",
    "        model=OCR_MODEL,\n",
    "        messages=[{'role': 'user', 'content': prompt, 'images': [img_base64]}],\n",
    "    )\n",
    "    return response[\"message\"][\"content\"].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "385fb0b5-1aaa-4700-a7a5-9f1ded70c8ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ GLM-OCR processing: Guide of developing Tang Nano FPGA on Mac.pdf\n",
      "‚ùå OCR failed: Unable to get page count.\n",
      "I/O Error: Couldn't open file './pdfs/Guide of developing Tang Nano FPGA on Mac.pdf': No such file or directory.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Define the path of the PDF to process\n",
    "pdf_path = \"./pdfs/Guide of developing Tang Nano FPGA on Mac.pdf\"\n",
    "\n",
    "# Start of core logic (all code in the original function is expanded flat)\n",
    "full_text = \"\"  # Initialize the full text variable\n",
    "try:\n",
    "    # Get PDF file name and TXT save name\n",
    "    pdf_file_name = os.path.basename(pdf_path)\n",
    "    pdf_name = os.path.splitext(pdf_file_name)[0]\n",
    "    txt_file_name = pdf_name + \".txt\"\n",
    "    \n",
    "    # Print start prompt\n",
    "    print(f\"\\nüöÄ GLM-OCR processing: {pdf_file_name}\")\n",
    "    \n",
    "    # Convert PDF to images\n",
    "    pages = convert_from_path(\n",
    "        pdf_path, dpi=OCR_DPI, thread_count=1, use_pdftocairo=True, grayscale=True\n",
    "    )\n",
    "    total_pages = len(pages)\n",
    "\n",
    "    # Iterate through each page to process OCR\n",
    "    for idx, page_img in enumerate(pages):\n",
    "        # Print progress (every 5 pages / last page)\n",
    "        if (idx+1)%5 == 0 or (idx+1) == len(pages):\n",
    "            print(f\"   Recognizing: Page {idx+1}/{total_pages}\")\n",
    "        \n",
    "        page_text = \"\"\n",
    "        # Retry mechanism (max 2 attempts)\n",
    "        for retry in range(2):\n",
    "            try:\n",
    "                # Call OCR and write to TXT\n",
    "                with open(txt_file_name, \"a\", encoding=\"utf-8\") as f:\n",
    "                    page_text = ocr_single_page(page_img, OCR_PROMPT)\n",
    "                    f.write(page_text)\n",
    "                break  # Exit retry if successful\n",
    "            except TimeoutError:\n",
    "                print(f\"   ‚ö†Ô∏è Page {idx+1} OCR timed out, retrying {retry+1}...\")\n",
    "                time.sleep(2)\n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ö†Ô∏è Page {idx+1} OCR failed: {e}\")\n",
    "                time.sleep(2)\n",
    "        \n",
    "        # Fill placeholder text when OCR fails\n",
    "        if not page_text:\n",
    "            page_text = f\"[Recognition failed for Page {idx+1}]\"\n",
    "        \n",
    "        # Concatenate text\n",
    "        full_text += page_text + \"\\n\\n\"\n",
    "        \n",
    "\n",
    "    # Clean up redundant whitespace characters\n",
    "    full_text = re.sub(r'\\s+', ' ', full_text).strip()\n",
    "    print(f\"‚úÖ OCR completed! Text length: {len(full_text)} characters\")\n",
    "\n",
    "# Global exception handling\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå OCR failed: {str(e)}\")\n",
    "    full_text = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c16a055-f84d-4ec7-b646-19da97b1851f",
   "metadata": {},
   "source": [
    "### Get the category and tags\n",
    "After OCR, we can use the first 1000 words to get the category and tags of file. These will help better to find the related data.\n",
    "\n",
    "Sending first 1000 words to LLM to get the category and tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "804c1f89-5155-44b8-81a6-689063b4b609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the LLM model name for generating category lists (custom model name for ollama)\n",
    "LLM_MODEL = \"gpt-oss:20b\"\n",
    "\n",
    "# Construct prompt to extract categories from PDF text:\n",
    "# - Require output to be ONLY a dictionary with \"categories\" key (list value)\n",
    "# - No extra text/formatting outside the dictionary\n",
    "# - Use first 1000 chars of OCR-extracted full text as input content\n",
    "prompt = f\"\"\"\n",
    "Based on the following content, output only Numpy Dict with no other content:\n",
    "{{\"categories\":[\"...\"]}}\n",
    "\n",
    "Content:\n",
    "{full_text[:1000]}\n",
    "\"\"\"\n",
    "\n",
    "# Call ollama chat API to generate the category dictionary using the specified LLM model\n",
    "result = ollama.chat(model=LLM_MODEL, messages=[{\"role\":\"user\",\"content\":prompt}])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfabb7b-5dcd-430a-ac54-ca12d2be7779",
   "metadata": {},
   "source": [
    "Now we get the categories via Numpy Dict format string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18d8cc8a-0ded-4696-9938-2c9f85c00816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'categories': []}\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "cat_tags = ast.literal_eval(result[\"message\"][\"content\"])\n",
    "print(cat_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be371d5-6c3a-41a7-96af-09ef4ccb0877",
   "metadata": {},
   "source": [
    "Then we create a categories_list:\n",
    ">Why we not directly ask LLM generate like this. Because it is not stable and not easy to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3860c922-32d8-4bd4-9977-b330e0c9d568",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories_list = {}\n",
    "for category in cat_tags[\"categories\"]:\n",
    "    if category not in categories_list:\n",
    "        categories_list[category] = []\n",
    "    categories_list[category].append(pdf_path)\n",
    "    \n",
    "categories_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74684575-c400-42a3-b76e-ffd5cbdc2fe9",
   "metadata": {},
   "source": [
    "### Spliting\n",
    "Before embedding, we need sperate content to chunks.\n",
    "> Chunk means it maybe large than one sentence.\n",
    "\n",
    "Why split the content to chunk?\n",
    "\n",
    "Back to the question `What is Log curve?`. When you ask this question, RAG will calculate the similarity between question `What is Log curve?` and some content (we will use the category and tags to help). If not spliting, we will ask LLM with whole related PDF files, not just related text. It may be over the limit of context of LLM and embeddinggemma, so we need to split the text to chunk and try our best to give more content.\n",
    "> The limit of content of embeddinggemma is 2K, but we can't ask it with 2,000 words. Because the limit contain the answer and we may pass some chunks (not just 1 chunk). So give LLM space to answer.\n",
    "\n",
    "In below function, we will split text about 500 words size by `[\"\\n\\n\", \"\\n\", \"„ÄÇ\", \"ÔºÅ\", \"Ôºü\", \"Ôºõ\", \"Ôºå\", \"„ÄÅ\", \".\", \"!\", \"?\", \";\", \",\"]`. In case split text in middle of sentence or word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b70865b7-e105-4010-9d77-11a23619d15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_chunks(text, chunk_size=500, chunk_overlap=50):\n",
    "    \"\"\"\n",
    "    Split long text into semantically coherent chunks with natural language boundary handling\n",
    "    (avoids mid-sentence splits by prioritizing common separators)\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text to split\n",
    "        chunk_size (int): Max character length per chunk (default: 500)\n",
    "        chunk_overlap (int): Overlapping chars between chunks (default: 50)\n",
    "    \n",
    "    Returns:\n",
    "        list[str]: Non-empty, stripped text chunks\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    text_length = len(text)\n",
    "    # Priority separators (semantic order: paragraph > sentence > punctuation)\n",
    "    separators = [\"\\n\\n\", \"\\n\", \"„ÄÇ\", \"ÔºÅ\", \"Ôºü\", \"Ôºõ\", \"Ôºå\", \"„ÄÅ\", \".\", \"!\", \"?\", \";\", \",\"]\n",
    "\n",
    "    while start < text_length:\n",
    "        end = start + chunk_size\n",
    "        # Add remaining text as last chunk if end exceeds text length\n",
    "        if end >= text_length:\n",
    "            chunks.append(text[start:].strip())\n",
    "            break\n",
    "        \n",
    "        temp_chunk = text[start:end]\n",
    "        split_pos = -1\n",
    "        # Find last valid separator (after overlap threshold)\n",
    "        for sep in separators:\n",
    "            pos = temp_chunk.rfind(sep)\n",
    "            if pos != -1 and pos > (chunk_size - chunk_overlap):\n",
    "                split_pos = pos\n",
    "                break\n",
    "        \n",
    "        # Split at natural separator if found\n",
    "        if split_pos != -1:\n",
    "            chunk_end = start + split_pos + 1\n",
    "            chunks.append(text[start:chunk_end].strip())\n",
    "            start = chunk_end - chunk_overlap\n",
    "        # Fallback: split at chunk size with overlap\n",
    "        else:\n",
    "            chunks.append(temp_chunk.strip())\n",
    "            start = end - chunk_overlap\n",
    "\n",
    "    # Filter out empty chunks\n",
    "    return [c for c in chunks if c]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba5109d-8092-4a95-87b3-c3fdf6dda047",
   "metadata": {},
   "source": [
    "What is `chunk_overlap=50` meaning? \n",
    "\n",
    "`chunk_overlap` refers to the overlap length of text chunks, and its core purpose is to avoid semantic breaks caused by the abrupt truncation of text by preserving a section of overlapping content between adjacent text chunks, thus ensuring the coherence of context.\n",
    "\n",
    "For example, after spliting, the length and content of first 2 chunks like below. You can see the ending of the first chunk `suitable for learning and putting into production,` is same as the beginning of the second chunk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc8d7c27-2a8d-4393-a9b7-621cb24366a7",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m chunks = split_text_chunks(full_text)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mFirst chunk length: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[43mchunks\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mFirst chunk content:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mchunks[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mSecond chunk length: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(chunks[\u001b[32m1\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mIndexError\u001b[39m: list index out of range"
     ]
    }
   ],
   "source": [
    "chunks = split_text_chunks(full_text)\n",
    "print(f'First chunk length: {len(chunks[0])}')\n",
    "print(f'First chunk content:\\n{chunks[0]}\\n')\n",
    "\n",
    "print(f'Second chunk length: {len(chunks[1])}')\n",
    "print(f'Second chunk content:\\n{chunks[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2792d5-7dbd-45de-a8a5-f7f9112c743a",
   "metadata": {},
   "source": [
    "### Embedding\n",
    "Now we can embedding the content. New version Ollama has function to do it, we just need to set the model name, like below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a65d76-b748-479d-a319-81c4c4137362",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_MODEL = \"embeddinggemma\"\n",
    "\n",
    "embedding_result = ollama.embed(model=EMBEDDING_MODEL, input=full_text[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8361712a-a430-4bad-a8b0-c85ad0af1eee",
   "metadata": {},
   "source": [
    "After embedding, checking the vector. The default length of generated vector is 768:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae413c1-2413-4894-88b0-e5d2fc6473f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of embedded vector: 768\n",
      "[-0.1795711, -0.014000532, 0.014086971, 0.020606868, 0.068530254, 0.042396046, -0.020073084, 0.021680696, 0.025249122, -0.039366826, 0.010949659, -0.05269665, 0.030286899, -0.01697931, 0.10812019, 0.014858677, 0.011771816, -0.026990928, -0.056859653, 0.009036296, 0.036543313, -0.0494702, 2.5608497e-06, 0.0112010455, 0.0136307785, 0.03635816, 0.02216107, -0.011967776, 0.020245204, -0.031006882, 0.029326187, -0.007984784, 0.027734365, -0.038165923, 0.0009162924, 0.051496238, 0.013571829, -0.06736813, 0.058040578, -0.011162806, -0.03662894, 0.060834225, -0.017807618, 0.010662863, -0.010802845, -0.024050152, -0.018274004, -0.056624085, -0.0406374, 0.02730967, 0.020485438, 0.03649262, -0.062459107, -0.0069091204, -0.01730355, -0.016423075, -0.05392794, -0.02272239, -0.014979191, 0.03398299, -0.03778724, -0.0021451856, 0.018650385, 0.011395624, 0.046265505, -0.02664927, -0.005651879, 0.02514516, 0.016128816, 0.28290936, -0.015962197, -0.040483765, -0.02541215, -0.021154609, 0.22535045, 0.04009595, -0.008112084, -0.07928618, 0.0117059015, -0.015704269, 0.0040111565, -0.026084958, 0.011032514, -0.022616783, 0.08032061, -0.008859176, -0.032929655, -0.024056293, 0.025661195, -0.031641416, 0.026101934, -0.03649817, 0.030188315, -0.07496514, 0.043840166, -0.058982898, -0.034342267, 0.06577746, -0.05164218, -0.021862503]\n"
     ]
    }
   ],
   "source": [
    "print(f'Size of embedded vector: {len(embedding_result[\"embeddings\"][0])}')\n",
    "print(embedding_result[\"embeddings\"][0][:100])  # Just show first 100 elements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f027348d-9ac5-479e-8d6a-1a04b8765b53",
   "metadata": {},
   "source": [
    "Convert all chunks to vector parallelly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49dce05-5de7-4948-a816-4069611cecf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Finished Embedding, Dims: (20, 768)\n",
      "[[ 0.00150642 -0.0393247  -0.03107799 ... -0.0295878  -0.00446127\n",
      "  -0.02752576]\n",
      " [-0.06393804 -0.01268001 -0.02310357 ... -0.01874642  0.04281209\n",
      "   0.03216418]\n",
      " [-0.0704116  -0.04516455  0.02668154 ...  0.00263846  0.03863657\n",
      "  -0.00353938]\n",
      " ...\n",
      " [-0.04021888 -0.03375006  0.05549837 ... -0.0137254  -0.06254524\n",
      "   0.01480906]\n",
      " [-0.08117593 -0.04744734  0.01369969 ... -0.03297434  0.01833711\n",
      "  -0.03613842]\n",
      " [-0.09259374  0.04772294 -0.02131703 ...  0.03189305  0.02579138\n",
      "   0.01827864]]\n"
     ]
    }
   ],
   "source": [
    "# Import concurrent execution modules for parallel embedding generation\n",
    "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed\n",
    "import numpy as np\n",
    "\n",
    "# Configuration for embedding generation\n",
    "EMBEDDING_MODEL = \"embeddinggemma\"  # Ollama embedding model name\n",
    "MAX_WORKERS = 16  # Max parallel workers (adjust based on CPU/GPU resources)\n",
    "\n",
    "def embed_batch(chunk, model_name):\n",
    "    \"\"\"\n",
    "    Generate embedding for a single text chunk using Ollama\n",
    "    Args:\n",
    "        chunk (str): Text chunk to embed\n",
    "        model_name (str): Name of Ollama embedding model\n",
    "    Returns:\n",
    "        np.array: Embedding vector (empty array if failed)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Call Ollama API to get embedding for the chunk\n",
    "        res = ollama.embed(model=model_name, input=chunk)\n",
    "        # Convert embedding to float32 numpy array for efficiency\n",
    "        return np.array(res[\"embeddings\"], dtype=np.float32)\n",
    "    except:\n",
    "        # Return empty array if embedding generation fails\n",
    "        return np.array([])\n",
    "\n",
    "def parallel_get_embeddings(chunks, model_name):\n",
    "    \"\"\"\n",
    "    Generate embeddings for multiple text chunks in parallel\n",
    "    Args:\n",
    "        chunks (list[str]): List of text chunks to embed\n",
    "        model_name (str): Name of Ollama embedding model\n",
    "    Returns:\n",
    "        np.array: 2D array of embeddings (empty array if all failed)\n",
    "    \"\"\"\n",
    "    all_emb = []\n",
    "    # Use process pool for parallel embedding (faster for CPU/GPU-bound tasks)\n",
    "    with ProcessPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        # Submit embedding tasks for all chunks\n",
    "        futs = [executor.submit(embed_batch, b, model_name) for b in chunks]\n",
    "        # Collect valid embedding results\n",
    "        for f in futs:\n",
    "            be = f.result()\n",
    "            if len(be) > 0:\n",
    "                all_emb.append(be[0])\n",
    "    \n",
    "    # Stack valid embeddings into 2D array and print shape\n",
    "    if all_emb:\n",
    "        final = np.vstack(all_emb)\n",
    "        print(f\"‚úÖ Finished Embedding, Dims: {final.shape}\")\n",
    "        return final\n",
    "    # Return empty array if no valid embeddings\n",
    "    return np.array([])\n",
    "\n",
    "# Generate embeddings for all text chunks in parallel\n",
    "embeddings = parallel_get_embeddings(chunks, EMBEDDING_MODEL)\n",
    "# Print embedding array to verify output\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0b5096-d15b-44ee-98ec-f18a28e2da4c",
   "metadata": {},
   "source": [
    "I recommend you to add file path together. It will helps you to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a9de19-103f-4adf-b952-2f3d10ceb1dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00150642 -0.0393247  -0.03107799 ... -0.0295878  -0.00446127\n",
      "  -0.02752576]\n",
      " [-0.06393804 -0.01268001 -0.02310357 ... -0.01874642  0.04281209\n",
      "   0.03216418]\n",
      " [-0.0704116  -0.04516455  0.02668154 ...  0.00263846  0.03863657\n",
      "  -0.00353938]\n",
      " ...\n",
      " [-0.04021888 -0.03375006  0.05549837 ... -0.0137254  -0.06254524\n",
      "   0.01480906]\n",
      " [-0.08117593 -0.04744734  0.01369969 ... -0.03297434  0.01833711\n",
      "  -0.03613842]\n",
      " [-0.09259374  0.04772294 -0.02131703 ...  0.03189305  0.02579138\n",
      "   0.01827864]]\n"
     ]
    }
   ],
   "source": [
    "full_embeddings = {}\n",
    "full_embeddings[pdf_path]=embeddings\n",
    "print(full_embeddings[pdf_path])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a390c112-9458-4150-9b7a-c01117fa7f0b",
   "metadata": {},
   "source": [
    "### Save\n",
    "Maybe you have question: Why return np.array?\n",
    "\n",
    "Because we will use `.npy` file store these vectors and reload in future. \n",
    "> You also can use `.npz` to reduce file size, or use JSON, Parquet, FAISS/Chroma, etc. Suit yourself.\n",
    "> \n",
    "> But `.npy` has one downsize: when you add new vectors to `.npy` file, it needs to load old vectors, concatenate together, and save. It can't add new vectors directly.\n",
    "\n",
    "We need to store many data: embedded vectors, categories and tags, respective paths.\n",
    "> If you are in development, I recommend you save chunks. Spliting spends much time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a84839-e753-4cc2-8498-05d8e824032b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'./pdfs/Guide of developing Tang Nano FPGA on Mac.pdf': array([[-0.09427399, -0.09588927,  0.06010244, ..., -0.00119149,\n",
      "         0.03862596,  0.01399344],\n",
      "       [-0.08799237, -0.01759241,  0.07082588, ..., -0.03582003,\n",
      "         0.02450423, -0.0444714 ],\n",
      "       [ 0.00150642, -0.0393247 , -0.03107799, ..., -0.0295878 ,\n",
      "        -0.00446127, -0.02752576],\n",
      "       ...,\n",
      "       [-0.04021888, -0.03375006,  0.05549837, ..., -0.0137254 ,\n",
      "        -0.06254524,  0.01480906],\n",
      "       [-0.08117593, -0.04744734,  0.01369969, ..., -0.03297434,\n",
      "         0.01833711, -0.03613842],\n",
      "       [-0.09259374,  0.04772294, -0.02131703, ...,  0.03189305,\n",
      "         0.02579138,  0.01827864]], shape=(22, 768), dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "print(full_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e708da85-f39f-4fb9-bc0b-75735c0b0930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will save data in `save` directory\n",
    "SAVE_DIR = \"save\"\n",
    "EMBEDDING_SAVE_PATH = os.path.join(SAVE_DIR, \"embeddings.npy\")\n",
    "\n",
    "# save embedded vectors\n",
    "np.save(EMBEDDING_SAVE_PATH, full_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833e426a-154a-4088-b1d0-93ef9db28dba",
   "metadata": {},
   "source": [
    "We also need to save categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017ee14a-9a76-46b4-983e-720727cf44c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will save data in `save` directory\n",
    "SAVE_DIR = \"save\"\n",
    "CATEGORIES_LIST_SAVE_PATH = os.path.join(SAVE_DIR, \"categories_list.npy\")\n",
    "\n",
    "np.save(CATEGORIES_LIST_SAVE_PATH, np.array(categories_list, dtype=object))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05530835-0cb9-4cb3-a9f8-590ea845f5ef",
   "metadata": {},
   "source": [
    "### Load\n",
    "Now we try to load `whole_info.npy` and `cat_tags.npy` file:\n",
    "> load `cat_tags.npy` is for find the closest keyword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ea1e90-876a-40b2-90d7-16e920423a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_DIR = \"save\"\n",
    "CATEGORIES_SAVE_PATH = os.path.join(SAVE_DIR, \"categories_list.npy\")\n",
    "EMBEDDING_SAVE_PATH = os.path.join(SAVE_DIR, \"embeddings.npy\")\n",
    "\n",
    "categories_list_loaded = np.load(CATEGORIES_SAVE_PATH, allow_pickle=True).item()\n",
    "embeddings_loaded = np.load(EMBEDDING_SAVE_PATH, allow_pickle=True).item() # .item() convert array to dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03afae31-2fbe-4e1d-8f14-6ea27628df6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'FPGA': ['./pdfs/Guide of developing Tang Nano FPGA on Mac.pdf'], 'Tang Nano': ['./pdfs/Guide of developing Tang Nano FPGA on Mac.pdf'], 'Mac': ['./pdfs/Guide of developing Tang Nano FPGA on Mac.pdf'], 'GOWIN IDE': ['./pdfs/Guide of developing Tang Nano FPGA on Mac.pdf'], 'Text Editor': ['./pdfs/Guide of developing Tang Nano FPGA on Mac.pdf'], 'Clang/LLVM': ['./pdfs/Guide of developing Tang Nano FPGA on Mac.pdf'], 'Hardware Development': ['./pdfs/Guide of developing Tang Nano FPGA on Mac.pdf'], 'Embedded Systems': ['./pdfs/Guide of developing Tang Nano FPGA on Mac.pdf'], 'Development Workflow': ['./pdfs/Guide of developing Tang Nano FPGA on Mac.pdf'], 'Open Source': ['./pdfs/Guide of developing Tang Nano FPGA on Mac.pdf']}\n",
      "\n",
      "{'./pdfs/Guide of developing Tang Nano FPGA on Mac.pdf': array([[-0.09427399, -0.09588927,  0.06010244, ..., -0.00119149,\n",
      "         0.03862596,  0.01399344],\n",
      "       [-0.08799237, -0.01759241,  0.07082588, ..., -0.03582003,\n",
      "         0.02450423, -0.0444714 ],\n",
      "       [ 0.00150642, -0.0393247 , -0.03107799, ..., -0.0295878 ,\n",
      "        -0.00446127, -0.02752576],\n",
      "       ...,\n",
      "       [-0.04021888, -0.03375006,  0.05549837, ..., -0.0137254 ,\n",
      "        -0.06254524,  0.01480906],\n",
      "       [-0.08117593, -0.04744734,  0.01369969, ..., -0.03297434,\n",
      "         0.01833711, -0.03613842],\n",
      "       [-0.09259374,  0.04772294, -0.02131703, ...,  0.03189305,\n",
      "         0.02579138,  0.01827864]], shape=(22, 768), dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "print(f'{cat_tags_loaded}\\n')\n",
    "print(embeddings_loaded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac6fa4a-483f-483c-b3c4-91d5fa3d1a50",
   "metadata": {},
   "source": [
    "## Finding related data\n",
    "Ok, we arrive at the core of RAG: Finding related data.\n",
    "\n",
    "This step can be devided to 5 samll steps:\n",
    "1. Try to find categories, tags (This step can narrow down the search scope), \n",
    "2. Embed your question,\n",
    "3. Find the related chunks by similarity.\n",
    "\n",
    "Maybe you noticed, it works like a search engine. So you can modify it to a local search engine. Without exact match, you can find where it appear and in which file. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e1bc5f-43e6-4a04-90b4-b0439b9ada54",
   "metadata": {},
   "source": [
    "### Try to find categories, tags\n",
    "Back to the question `What is FPGA?`, finding the closest categories. \n",
    "\n",
    "#### First Method\n",
    "It counts the number of full occurrences of each preset keyword in the lowercase text chunk as the matching score for that keyword, and returns the keyword with the highest score as the classification result (or \"General Document\" if no keywords match)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ac990c-f0d9-4952-ab6b-a6b2c3ffe9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_text_chunk(chunk):    \n",
    "    \"\"\"\n",
    "    Classify text chunk by keyword count (case-insensitive)\n",
    "    Args:\n",
    "        chunk (str): Text chunk to classify\n",
    "    Returns:\n",
    "        str: Top matching category or \"General Document\" (no matches)\n",
    "    \"\"\"\n",
    "    # Normalize chunk to lowercase for consistent matching\n",
    "    chunk_lower = chunk.lower()\n",
    "    # Initialize score tracker for all predefined categories\n",
    "    category_scores = {cat: 0 for cat in categories_list_loaded}\n",
    "\n",
    "    # Count category keyword occurrences in chunk\n",
    "    for category in categories_list_loaded:\n",
    "        category_scores[category] = chunk_lower.count(category.lower())\n",
    "    \n",
    "    # Get highest score and return corresponding category (or default)\n",
    "    max_score = max(category_scores.values())\n",
    "    return max(category_scores, key=category_scores.get) if max_score > 0 else \"General Document\"\n",
    "    \n",
    "def classify_chunks(text):\n",
    "    \"\"\"\n",
    "    Batch classify list of text chunks\n",
    "    Args:\n",
    "        text (list[str]): List of text chunks\n",
    "    Returns:\n",
    "        list[str]: Categories for each chunk\n",
    "    \"\"\"\n",
    "    # Apply single-chunk classification to all chunks\n",
    "    return [classify_text_chunk(c) for c in text]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64030a9-1977-473f-8d25-23537e0e835e",
   "metadata": {},
   "source": [
    "Try it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907c888a-566b-455d-9062-f9eb897753f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FPGA\n"
     ]
    }
   ],
   "source": [
    "question=\"What is FPGA?\"\n",
    "keyword = classify_text_chunk(question)\n",
    "print(keyword)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c559a53e-adb1-4987-8a66-91cd0240377c",
   "metadata": {},
   "source": [
    "This algorithm is very simple: we just get 1 keyword. It is for showing. In actual, I recommend you to get some keywords, it will works better.\n",
    "\n",
    "Now we can get this keyword embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a095abee-f88a-4f1d-88b2-4a3e57d38349",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./pdfs/Guide of developing Tang Nano FPGA on Mac.pdf']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_tags_loaded[keyword]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbee655-476d-4846-95ee-9320d23830df",
   "metadata": {},
   "source": [
    "#### Second Method\n",
    "Or you can generate embedding vectors to calculate similarity.\n",
    "\n",
    "To calculate similarity, we need to calculate cosine similarity of two tensors or vectors. Cosine similarity means the angle between two vectors by cosine. \n",
    "\n",
    "$$\n",
    "\\text{cosine similarity} := \\cos(\\theta) = \\frac{\\mathbf{A} \\cdot \\mathbf{B}}{\\|\\mathbf{A}\\| \\|\\mathbf{B}\\|} = \\frac{\\sum_{i=1}^{n} A_i B_i}{\\sqrt{\\sum_{i=1}^{n} A_i^2} \\cdot \\sqrt{\\sum_{i=1}^{n} B_i^2}}\n",
    "$$\n",
    "\n",
    "Why we just calculate the distance of two vectors? Because the range of cosine similarity is 0 to 1, satisfies the axioms of probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fb0964-60a2-4abd-a4d2-f17d92405581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FPGA                 0.722461246440\n",
      "Tang Nano            0.416567403635\n",
      "Mac                  0.486751758428\n",
      "GOWIN IDE            0.425685658280\n",
      "Text Editor          0.422178165151\n",
      "Clang/LLVM           0.492924708220\n",
      "Hardware Development 0.623216937434\n",
      "Embedded Systems     0.624374652789\n",
      "Development Workflow 0.494293051459\n",
      "Open Source          0.465430764758\n",
      "\n",
      "Best Category: FPGA\n"
     ]
    }
   ],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"\n",
    "    Calculate cosine similarity between two vectors (normalized dot product)\n",
    "    Args:\n",
    "        vec1 (np.array): First embedding vector\n",
    "        vec2 (np.array): Second embedding vector\n",
    "    Returns:\n",
    "        float: Cosine similarity score (0.0 if either vector is zero)\n",
    "    \"\"\"\n",
    "    # Compute L2 norm of each vector (magnitude)\n",
    "    n1 = np.linalg.norm(vec1)\n",
    "    n2 = np.linalg.norm(vec2)\n",
    "    \n",
    "    # Return 0 if either vector has zero norm (avoid division by zero)\n",
    "    if n1 == 0 or n2 == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # Calculate normalized dot product (cosine similarity)\n",
    "    return np.dot(vec1/n1, vec2/n2)\n",
    "\n",
    "# Initialize variables to track top matching category\n",
    "max_similarity = -1.0  # Track highest similarity score\n",
    "best_category = \"\"     # Track category with highest similarity\n",
    "    \n",
    "# Generate embedding for user's question\n",
    "question_embedding = ollama.embed(model=EMBEDDING_MODEL, input=question)[\"embeddings\"][0]\n",
    "\n",
    "# Iterate through all categories to find most similar one\n",
    "for category in categories_list_loaded:\n",
    "    # Generate embedding for current category and calculate similarity to question\n",
    "    similarity = cosine_similarity(question_embedding, ollama.embed(model=EMBEDDING_MODEL, input=category)[\"embeddings\"][0])\n",
    "    \n",
    "    # Print category (left-aligned) and similarity (12 decimal places)\n",
    "    print(f\"{category:<{20}} {similarity:.{12}f}\")\n",
    "    \n",
    "    # Update top category if current similarity is higher\n",
    "    if similarity > max_similarity:\n",
    "        max_similarity = similarity\n",
    "        best_category = category\n",
    "\n",
    "# Print the final best-matching category\n",
    "print(f'\\nBest Category: {best_category}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c274ee34-ac16-4be9-ba17-406327359753",
   "metadata": {},
   "source": [
    "### Embed question\n",
    "This step is easy, we meet it many times before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc03453-ea62-41fb-a313-e1af0b292dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.15408693, -0.026008384, -0.020546477, 0.0059912973, 0.019676102, -0.011539707, -0.0008004644, 0.012970376, 0.030865138, 0.048791505, -0.01950369, -0.044546984, 0.022569103, -0.0025020633, 0.055862352, 0.019775027, 0.07262218, -0.013940342, -0.032102257, -0.034715135, 0.0053905854, -0.013554472, -0.012396905, 0.053742614, 0.0040722643, 0.0032516464, 0.0068892143, -0.06406595, -0.029328734, 0.009482054, 0.0413703, 0.0016903707, 0.044082124, -0.010227873, -0.0008136854, 0.040183045, -0.011929971, -0.047931533, 0.010604784, -0.004251502, -0.009085997, 0.08101945, -0.0062629674, -0.0031834245, -0.0057065976, -0.06732319, -0.044221506, -0.06720558, 0.009620878, -0.012304886, -0.0037627423, -0.015941259, -0.013720847, 0.034674466, -0.0066491696, 0.023288716, -0.053317003, 0.0061896513, 0.02162611, 0.05568433, -0.046298698, -0.0016431058, 0.010757171, -0.027773952, 0.024997184, 0.04797287, -0.02842763, 0.005134202, -0.0047964654, 0.18524532, 0.013712985, -0.01291761, -0.060923137, -0.036617257, 0.1864239, 0.11463216, -0.05500811, 0.01972159, 0.0063644047, -0.012182781, 0.041780893, 0.003348158, 0.0047385967, -0.02165367, 0.12178857, 0.022519832, -0.03999698, 0.00430564, 0.039600242, -0.06515105, 0.04400481, 0.0123990895, -0.07381522, -0.044898994, -0.011210072, -0.03791841, -0.028023474, -0.018522812, -0.05928535, -0.04325633]\n"
     ]
    }
   ],
   "source": [
    "question_embedding = ollama.embed(model=EMBEDDING_MODEL, input=question)[\"embeddings\"][0]\n",
    "print(question_embedding[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af01be95-3ddd-4c4e-b40b-2c6d141a0b76",
   "metadata": {},
   "source": [
    "### Find the related chunks by similarity\n",
    "Ok, finally, we arrive at the core of RAG. I have shown all knowlegde needed before, so write a loop to find closest chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66d494e-827f-4848-920a-418daa010b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2854506568758853\n",
      "0.4253609639088747\n",
      "0.20651224128702755\n",
      "0.3126381882554131\n",
      "0.3249318263069938\n",
      "0.38123328018590286\n",
      "0.34579035236468514\n",
      "0.17157891123196545\n",
      "0.363234404828212\n",
      "0.41283551366441573\n",
      "0.23944850963462253\n",
      "0.38228374480729826\n",
      "0.42458171746203693\n",
      "0.44611595463853976\n",
      "0.27074020687240574\n",
      "0.33667093007448123\n",
      "0.22012099728769222\n",
      "0.3128852069444872\n",
      "0.4832363620397254\n",
      "0.41481237150705463\n",
      "0.2927572640138574\n",
      "0.26775386458528444\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for file in embeddings_loaded:\n",
    "    index = 0\n",
    "    for idx, embedding in enumerate(embeddings_loaded[file]):\n",
    "        similarity = cosine_similarity(question_embedding, embedding)\n",
    "        results.append((idx, similarity, chunks[idx]))\n",
    "        print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13017d6-6272-49f9-89fb-15dc297a086c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(18,\n",
       "  np.float64(0.4832363620397254),\n",
       "  '| Sipeed Tang Nano: | GW1N-LV1QN48C6/I5 | tangnano | | Sipeed Tang Nano 1K | GW1NZ-LV1QN48C6/I5 | tangnano1k | | Sipeed Tang Nano 4K | GW1NSR-LV4CQN48PC7/I6 | tangnano4k | | Sipeed Tang Nano 9K | GW1NR-LV9QN88PC6/I5 | tangnano9k | | Seeed RUNBER | GW1N-UV4LQ144C6/I5 | runber | | @Disasm honeycomb | GW1NS-UX2CQN48C5/I4 | honeycomb | Next, if you have a Tang nano 9K like me, then use the following command (the device cannot write the long one in table): ```bash $ gowin_pack -d GW1N-9C -o top.'),\n",
       " (13,\n",
       "  np.float64(0.44611595463853976),\n",
       "  's @(posedge clk) begin clockCounter <= clockCounter + 1; if (clockCounter == WAIT_TIME) begin clockCounter <= 0; cur_state <= cur_state << 1; if (cur_state == 6\\'b000000) begin ``` end end assign led[5:0] = cur_state[5:0]; endmodule // tangnano9k.cst IO_LOC \"clk\" 52; IO_PORT \"clk\" PULL_MODE=UP; IO_LOC \"led[0]\" 10; IO_LOC \"led[1]\" 11; IO_LOC \"led[2]\" 13; IO_LOC \"led[3]\" 14; IO_LOC \"led[4]\" 15; IO_LOC \"led[5]\" 16; If use VS Code extension to set up tangnano9k.'),\n",
       " (1,\n",
       "  np.float64(0.4253609639088747),\n",
       "  'suitable for learning and putting into production, because IDE hides many details. So I want to find other workflows, just like using any text editor and Clang/LLVM to compile C/C++ programs. What is being developed in FPGA development? First of all, you need to know what is being developed in FPGA development, and you can find the tools and software needed. The essence of the computing machine executing various instructions is to energize the pins of series of logic units,')]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort the results\n",
    "results.sort(key=lambda x: x[1], reverse=True)\n",
    "# Show top 5 chunks idx and similarity\n",
    "results[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9b3c27-b7e1-47c9-9d36-8e4e784de99a",
   "metadata": {},
   "source": [
    "## Ask LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8503893e-9dfa-47e7-8a52-8cc1ef0122c5",
   "metadata": {},
   "source": [
    "We arrive at the end of the RAG. Just ask LLM with above selected chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016a606a-882a-4c2b-9af2-cc95a3975d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(query, relevant_chunks):\n",
    "    if not relevant_chunks:\n",
    "        return \"No relevant content found\"\n",
    "    # concentrate Top 5 chunks together \n",
    "    ctx = \"\\n\\n\".join([f\"{item[1][2]}\" for item in enumerate(relevant_chunks)])\n",
    "    prompt = f\"\"\"Please answer truthfully based on the context, prioritize extracting specific numbers and details, say 'I don't know' if you don't know, and do not fabricate content.\n",
    "Context:\n",
    "{ctx}\n",
    "Question: {query}\"\"\"\n",
    "    try:\n",
    "        resp = ollama.chat(model=LLM_MODEL, messages=[{\"role\":\"user\",\"content\":prompt}])\n",
    "        return resp[\"message\"][\"content\"]\n",
    "    except:\n",
    "        return \"Failed to generate answer\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84592b9d-26fd-4b71-9051-fa349577e463",
   "metadata": {},
   "source": [
    "Let's try the question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cc54ee-67fa-4cbe-9fda-528a8c150035",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'**FPGA** stands for **Field‚ÄëProgrammable Gate Array**.  \\nIt is a type of integrated circuit that contains an array of configurable logic blocks (CLBs), input/output (I/O) cells, programmable routing resources, and sometimes embedded memory or DSP blocks. Unlike fixed‚Äëlogic ASICs, an FPGA can be programmed after manufacture by loading a configuration file (often called a *bitstream*) that tells the device how to interconnect its internal blocks to implement a desired digital circuit.\\n\\nKey points from the context:\\n\\n| Device name | Part number | Board name | FPGA family |\\n|-------------|-------------|------------|-------------|\\n| Sipeed Tang Nano 9K | **GW1NR‚ÄëLV9QN88PC6/I5** | tangnano9k | Gowin GW1N‚Äë9C family |\\n| Sipeed Tang Nano 4K | GW1NSR‚ÄëLV4CQN48PC7/I6 | tangnano4k | Gowin GW1N‚Äë4C family |\\n| Sipeed Tang Nano 1K | GW1NZ‚ÄëLV1QN48C6/I5 | tangnano1k | Gowin GW1N‚Äë1C family |\\n| Sipeed Tang Nano | GW1N‚ÄëLV1QN48C6/I5 | tangnano | Gowin GW1N‚Äë1C family |\\n| Seeed RUNBER | GW1N‚ÄëUV4LQ144C6/I5 | runber | Gowin GW1N‚Äë4C family |\\n| @Disasm honeycomb | GW1NS‚ÄëUX2CQN48C5/I4 | honeycomb | Gowin GW1N‚Äë4C family |\\n\\nTo use an FPGA:\\n\\n1. **Write HDL code** (e.g., Verilog or VHDL) that describes the desired logic.  \\n2. **Synthesize** the HDL to a netlist.  \\n3. **Place‚Äëand‚Äëroute** (PnR) to generate a device‚Äëspecific configuration file (bitstream).  \\n4. **Pack** the bitstream for the target device (`gowin_pack -d GW1N‚Äë9C -o top.fs test_pnr.json`).  \\n5. **Burn** the bitstream to the FPGA (e.g., via JTAG).  \\n\\nThe result is a programmable, reconfigurable digital system that can be tailored to many applications, from simple LED lighting (as shown in the example `top.v`) to complex processors, accelerators, or custom logic blocks.'"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question=\"What is FPGA?\"\n",
    "relevant_chunks = results[:5]\n",
    "generate_answer(question, relevant_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d9bb1a-9820-4768-9d89-d8295cc21fea",
   "metadata": {},
   "source": [
    "Ask a question which isn't about this PDF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f89fc84-8e2c-4350-88c1-4e3c542d663e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't know.\""
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question=\"What is Log curve?\"\n",
    "relevant_chunks = results[:5]\n",
    "generate_answer(question, relevant_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b31bbc-c392-43de-8d25-40d3032c88e2",
   "metadata": {},
   "source": [
    "The LLM will not answer with lies.\n",
    "\n",
    "Next, we will make it auto loads all PDF files and processes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
